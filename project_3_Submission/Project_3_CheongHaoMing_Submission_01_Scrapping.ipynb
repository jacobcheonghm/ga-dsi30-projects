{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9e7205-5e30-42ea-b86d-bd20647408cc",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP\n",
    "\n",
    "DSI30 Submission by Cheong Hao Ming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c8f42-be7a-4844-a4c6-dd1a2f0c11d4",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Running Lab is a home grown running specialty store that retails everything running. Despite the recent pandemic, running, more specifically marathon running has been gaining much popularity among Singaporeans. Marathon runners has long been a target group for Running Lab as this group of runners tend to spend the most time and money in the sport.\n",
    "\n",
    "Therefore, Running Lab would like to learn more about first time marathon runners while navigating through the chatter of the general running discussions and more marathon specific content. In fact, they would like to deep dive into the discussions of first time marathon runners to bring in the appropriate products and services catering to this group of customers.\n",
    "\n",
    "As such, the management of Runnining Lab has commissioned our consultancy to create a classification model to seperate the generic running discussion and discussions pertaining to first time marathoners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14ddec7-ea19-4065-982d-2edd880e2391",
   "metadata": {},
   "source": [
    "## Data Collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff6b062-da5f-4417-a3c8-6c8c9929084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary librarys/packages\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b744a-8956-464d-b42d-0854f95d3458",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pulling data from Running Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86030641-7c18-490f-a538-7a6a8e28f4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration 1.\n",
      "Status Code:  200\n",
      " Iteration 2.\n",
      "Status Code:  200\n",
      " Iteration 3.\n",
      "Status Code:  200\n",
      " Iteration 4.\n",
      "Status Code:  200\n",
      " Iteration 5.\n",
      "Status Code:  200\n",
      " Iteration 6.\n",
      "Status Code:  200\n",
      " Iteration 7.\n",
      "Status Code:  200\n",
      " Iteration 8.\n",
      "Status Code:  200\n",
      " Iteration 9.\n",
      "Status Code:  200\n",
      " Iteration 10.\n",
      "Status Code:  200\n",
      " Iteration 11.\n",
      "Status Code:  200\n",
      " Iteration 12.\n",
      "Status Code:  200\n",
      " Iteration 13.\n",
      "Status Code:  200\n",
      " Iteration 14.\n",
      "Status Code:  200\n",
      " Iteration 15.\n",
      "Status Code:  200\n",
      " Iteration 16.\n",
      "Status Code:  200\n",
      " Iteration 17.\n",
      "Status Code:  200\n",
      " Iteration 18.\n",
      "Status Code:  200\n"
     ]
    }
   ],
   "source": [
    "utc = 1658223227 #set the utc for first loop as (Tuesday, July 19, 2022 5:33:47 PM)\n",
    "post_list = [] #empty list to host the posts\n",
    "\n",
    "for i in range(18): #run the loop 18 times to get 1800 posts\n",
    "    print(f\" Iteration {i+1}.\")\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    'subreddit':'running', #define the subreddit\n",
    "    'size':100, #maximum scrapping size\n",
    "    'before':utc} #set 'before' params to ensure no duplicates after each loop\n",
    "    res = requests.get(url,params)\n",
    "    print('Status Code: ',res.status_code)\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    utc = posts[-1]['created_utc'] #set utc to be of the last post scrapped\n",
    "    post_list.append(posts)\n",
    "    time.sleep(random.randint(3,9)) #pause the loop to simulate a more human-like process\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2425124-0cf5-4452-a33a-9f5c78f8b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining data from 18 iterations into a single list\n",
    "\n",
    "new_list = []\n",
    "\n",
    "for items in post_list:\n",
    "    for x in items:\n",
    "        new_list.append(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "256b0891-0e5e-41e7-bdac-40ed75ec58b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking list size\n",
    "len(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96364aff-b9fe-4e8a-9fe2-f91efbbfdf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting list into DataFrame\n",
    "run = pd.DataFrame(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b309ecc3-c2e8-4f44-a398-8dcb1b6745a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  1800 non-null   object\n",
      " 1   selftext   1799 non-null   object\n",
      " 2   title      1800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 42.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#quick check on null and dtype\n",
    "run[['subreddit','selftext','title']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce89f736-64d1-4c5d-90af-ee3b830d73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine DataFrame with only the required columns\n",
    "run = run[['subreddit','selftext','title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4751d69-6b67-401f-8f08-3abee8ba0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as csv file for processing later\n",
    "run.to_csv(\"running.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98512a-83d1-4689-8772-d9d41d4ec81c",
   "metadata": {},
   "source": [
    "### Pulling data from FirstMarathon Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a07cdc-8ac1-42f9-bb46-4cb6aa074f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration 1.\n",
      "Status Code:  200\n",
      " Iteration 2.\n",
      "Status Code:  200\n",
      " Iteration 3.\n",
      "Status Code:  200\n",
      " Iteration 4.\n",
      "Status Code:  200\n",
      " Iteration 5.\n",
      "Status Code:  200\n",
      " Iteration 6.\n",
      "Status Code:  200\n",
      " Iteration 7.\n",
      "Status Code:  200\n",
      " Iteration 8.\n",
      "Status Code:  200\n",
      " Iteration 9.\n",
      "Status Code:  200\n",
      " Iteration 10.\n",
      "Status Code:  200\n",
      " Iteration 11.\n",
      "Status Code:  200\n",
      " Iteration 12.\n",
      "Status Code:  200\n",
      " Iteration 13.\n",
      "Status Code:  200\n",
      " Iteration 14.\n",
      "Status Code:  200\n",
      " Iteration 15.\n",
      "Status Code:  200\n",
      " Iteration 16.\n",
      "Status Code:  200\n",
      " Iteration 17.\n",
      "Status Code:  200\n",
      " Iteration 18.\n",
      "Status Code:  200\n"
     ]
    }
   ],
   "source": [
    "utc = 1658223227 #set the utc for first loop as (Tuesday, July 19, 2022 5:33:47 PM)\n",
    "post_list_1 = [] #empty list to host the posts\n",
    "\n",
    "for i in range(18): #run the loop 18 times to get 1800 posts\n",
    "    print(f\" Iteration {i+1}.\")\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {\n",
    "    'subreddit':'firstmarathon', #define the subreddit\n",
    "    'size':100, #maximum scrapping size\n",
    "    'before':utc} #set 'before' params to ensure no duplicates after each loop\n",
    "    res = requests.get(url,params)\n",
    "    print('Status Code: ',res.status_code)\n",
    "    data = res.json()\n",
    "    posts = data['data']\n",
    "    utc = posts[-1]['created_utc'] #set utc to be of the last post scrapped\n",
    "    post_list_1.append(posts)\n",
    "    time.sleep(random.randint(3,9)) #pause the loop to simulate a more human-like process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a0b7e78-c093-4283-b3c0-6b499830dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining data into a single list\n",
    "new_list_1 = []\n",
    "\n",
    "for items in post_list_1:\n",
    "    for x in items:\n",
    "        new_list_1.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac9ff59f-c0ae-47cb-b615-436495942eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking size of the list\n",
    "len(new_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed45582-9f24-4e20-a3be-2565c7f44b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the list into a DataFrame\n",
    "mar = pd.DataFrame(new_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "791183bc-73d9-41d1-b1a1-2173532329ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  1800 non-null   object\n",
      " 1   selftext   1795 non-null   object\n",
      " 2   title      1800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 42.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#quick check on null and dtype\n",
    "mar[['subreddit','selftext','title']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d58c8e4-d617-4314-9a8b-4d7781813cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine DataFrame with only the required columns\n",
    "mar = mar[['subreddit','selftext','title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c6ed30f-b176-47c7-82c0-8740e55932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv for processing later\n",
    "mar.to_csv(\"firstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c78bf3-67db-409d-a099-0dca02a914d1",
   "metadata": {},
   "source": [
    "Preprocessing, Modelling, Evaluationa and Conclusion will be continued in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95029c6d-2701-4d87-ac26-9eddadfcb4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
